{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6264a779-aff5-43c9-856e-d8294103b03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\gunta\\anaconda3\\envs\\my_env\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Downloading scipy-1.10.1-cp38-cp38-win_amd64.whl (42.2 MB)\n",
      "     ---------------------------------------- 0.0/42.2 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/42.2 MB 660.6 kB/s eta 0:01:04\n",
      "     ---------------------------------------- 0.1/42.2 MB 1.1 MB/s eta 0:00:41\n",
      "     ---------------------------------------- 0.3/42.2 MB 2.7 MB/s eta 0:00:16\n",
      "      --------------------------------------- 0.9/42.2 MB 5.0 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 1.5/42.2 MB 7.2 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 3.0/42.2 MB 11.1 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 3.5/42.2 MB 12.5 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 4.5/42.2 MB 12.4 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 5.8/42.2 MB 14.8 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 7.9/42.2 MB 17.3 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 8.1/42.2 MB 16.2 MB/s eta 0:00:03\n",
      "     --------- ----------------------------- 10.0/42.2 MB 18.3 MB/s eta 0:00:02\n",
      "     ---------- ---------------------------- 11.2/42.2 MB 25.2 MB/s eta 0:00:02\n",
      "     ----------- --------------------------- 12.2/42.2 MB 26.2 MB/s eta 0:00:02\n",
      "     ------------ -------------------------- 14.0/42.2 MB 27.3 MB/s eta 0:00:02\n",
      "     ------------- ------------------------- 14.9/42.2 MB 28.4 MB/s eta 0:00:01\n",
      "     --------------- ----------------------- 16.8/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------- ----------------------- 17.1/42.2 MB 26.2 MB/s eta 0:00:01\n",
      "     ---------------- ---------------------- 17.3/42.2 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------ -------------------- 19.5/42.2 MB 27.3 MB/s eta 0:00:01\n",
      "     ------------------ -------------------- 19.5/42.2 MB 27.3 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 21.6/42.2 MB 25.2 MB/s eta 0:00:01\n",
      "     -------------------- ------------------ 22.6/42.2 MB 25.2 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 23.7/42.2 MB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------- ---------------- 24.5/42.2 MB 22.6 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 25.8/42.2 MB 23.4 MB/s eta 0:00:01\n",
      "     ------------------------ -------------- 26.4/42.2 MB 21.1 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 27.5/42.2 MB 21.8 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 28.2/42.2 MB 22.6 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 29.5/42.2 MB 21.1 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 30.7/42.2 MB 22.6 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 31.8/42.2 MB 21.1 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 32.8/42.2 MB 22.6 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 33.9/42.2 MB 22.6 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 35.1/42.2 MB 21.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 36.2/42.2 MB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 37.1/42.2 MB 21.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 38.0/42.2 MB 22.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 39.2/42.2 MB 22.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 39.8/42.2 MB 21.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  41.2/42.2 MB 21.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  41.7/42.2 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 21.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 42.2/42.2 MB 20.5 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.9/9.3 MB 29.4 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.9/9.3 MB 24.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.7/9.3 MB 21.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.4/9.3 MB 20.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.8/9.3 MB 20.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.5/9.3 MB 19.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.8/9.3 MB 19.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.5/9.3 MB 19.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.2/9.3 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.3 MB 18.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 17.4 MB/s eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 302.2/302.2 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.2 scipy-1.10.1 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "# !python --version\n",
    "# !tensorflow --version\n",
    "# !pip install Path\n",
    "# !pip install tensorflow\n",
    "# !pip install opencv-python\n",
    "# !pip install tensorflow-hub\n",
    "# !pip install pydicom\n",
    "#!pip install pandas\n",
    "#!pip install matplotlib\n",
    "#!pip install tqdm\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da819add-471e-4afd-b0cf-3e04426b55d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import cv2 as cv\n",
    "from path import Path\n",
    "import os \n",
    "import glob\n",
    "import tensorflow_hub as hub\n",
    "import os \n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6074f3c-557e-4e3f-999a-f5067586c427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df= pd.read_csv('C:\\\\ML Data\\\\BrainTumorML\\\\train_labels.csv')\n",
    "sample_df = pd.read_csv('C:\\\\ML Data\\\\BrainTumorML\\\\sample_submission.csv')\n",
    "#train_df.head(1)\n",
    "#train_df.tail()\n",
    "#train_df.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51ee1e84-a914-44f4-ae61-604a734dbe7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dicom(path):\n",
    "    dicom=pydicom.read_file(path)\n",
    "    data=dicom.pixel_array\n",
    "    data=data-np.min(data)\n",
    "    if np.max(data) != 0:\n",
    "        data=data/np.max(data)\n",
    "    data=(data*255).astype(np.uint8)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7dad50a7-4204-41b5-a97e-fe45023678fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def listdirs(folder):\n",
    "    return [d for d in os.listdir(folder) if os.path.isdir(os.path.join(folder, d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "757d8240-0009-4f0a-8ad1-41ad72967e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = train_df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "595f54d4-3d60-4c9f-8f7a-b5f87f450c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 585/585 [14:12<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "train_dir='C:\\\\ML Data\\\\BrainTumorML\\\\train'\n",
    "trainset=[]\n",
    "trainlabel=[]\n",
    "trainidt=[]\n",
    "for i in tqdm(range(len(train_df))):\n",
    "    idt=train_df.loc[i,'BraTS21ID']\n",
    "    idt2=('00000'+str(idt))[-5:]\n",
    "    path=os.path.join(train_dir,idt2,'T1wCE')              \n",
    "    for im in os.listdir(path):\n",
    "        img=load_dicom(os.path.join(path,im)) \n",
    "        img=cv.resize(img,(64,64)) \n",
    "        image=img_to_array(img)\n",
    "        image=image/255.0\n",
    "        trainset+=[image]\n",
    "        trainlabel+=[train_df.loc[i,'MGMT_value']]\n",
    "        trainidt+=[idt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09a1cdff-c12e-4870-abdd-270251543cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [02:31<00:00,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "This is the first testidt 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dir='C:\\\\ML Data\\\\BrainTumorML\\\\test'\n",
    "testset=[]\n",
    "testidt=[]\n",
    "for i in tqdm(range(len(sample_df))):\n",
    "    idt=sample_df.loc[i,'BraTS21ID']\n",
    "    idt2=('00000'+str(idt))[-5:]\n",
    "    path=os.path.join(test_dir,idt2,'T1wCE')               \n",
    "    for im in os.listdir(path):   \n",
    "        img=load_dicom(os.path.join(path,im))\n",
    "        img=cv.resize(img,(64,64)) \n",
    "        image=img_to_array(img)\n",
    "        image=image/255.0\n",
    "        testset+=[image]\n",
    "        testidt+=[idt]\n",
    "print(testidt[0])\n",
    "#print(\"This is the first testIdt\" , testidt[0])\n",
    "firstEX = testidt[0]\n",
    "print(f\"This is the first testidt {firstEX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "399782d4-d3bf-48fb-8b65-ebf2dd99216b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(testidt, open('C:\\\\ML Data\\\\BrainTumorML\\\\test\\\\testIdt.label', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "382832e1-cd88-4082-801e-c334fda2e5b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y=np.array(trainlabel)\n",
    "Y_train=to_categorical(y)\n",
    "X_train=np.array(trainset)\n",
    "X_test=np.array(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36840d38-669b-4b64-a080-047dd90b44f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(filters=64,kernel_size=(4,4),input_shape=(64,64,1),activation='relu',kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Conv2D(filters=64,kernel_size=(4,4),activation='relu',kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(keras.layers.Dropout(0.20))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Conv2D(filters=64,kernel_size=(4,4),activation='relu',kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(100,activation=\"relu\",kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(2,\"softmax\"))\n",
    "\n",
    "# model = keras.models.Sequential()\n",
    "# model.add(keras.layers.Conv2D(filters=64, kernel_size=(4,4), input_shape=(64,64,1), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "# model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(keras.layers.BatchNormalization())\n",
    "# model.add(keras.layers.Conv2D(filters=64, kernel_size=(4,4), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "# model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(keras.layers.BatchNormalization())\n",
    "# model.add(keras.layers.Dropout(0.20))\n",
    "# model.add(keras.layers.Conv2D(filters=64, kernel_size=(4,4), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "# model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(keras.layers.Dropout(0.25))\n",
    "# model.add(keras.layers.BatchNormalization())\n",
    "# model.add(keras.layers.Flatten())\n",
    "# model.add(keras.layers.Dropout(0.20))\n",
    "# model.add(keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "# model.add(keras.layers.Dropout(0.25))\n",
    "# model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e738bf0-f14e-4472-9e3b-7ae91c3999c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 61, 61, 64)        1088      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 30, 30, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 30, 30, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 27, 27, 64)        65600     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 13, 13, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 13, 13, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 13, 13, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 10, 10, 64)        65600     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 5, 5, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 5, 5, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               160100    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 293358 (1.12 MB)\n",
      "Trainable params: 292974 (1.12 MB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4c79aa2-a4f4-44ba-976d-bd4209bccbfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer = \"RMSprop\",metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0c2de92-eb03-43b8-9b19-1a71ce241328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "callback = keras.callbacks.EarlyStopping(monitor='loss', patience=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dba083bc-8d87-4324-b54b-f65c1075bd6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1512/1512 [==============================] - 126s 83ms/step - loss: 0.6924 - accuracy: 0.5566\n",
      "Epoch 2/10\n",
      "1512/1512 [==============================] - 124s 82ms/step - loss: 0.6645 - accuracy: 0.5854\n",
      "Epoch 3/10\n",
      "1512/1512 [==============================] - 124s 82ms/step - loss: 0.6210 - accuracy: 0.6264\n",
      "Epoch 4/10\n",
      "1512/1512 [==============================] - 126s 83ms/step - loss: 0.5779 - accuracy: 0.6594\n",
      "Epoch 5/10\n",
      "1512/1512 [==============================] - 131s 86ms/step - loss: 0.5408 - accuracy: 0.6835\n",
      "Epoch 6/10\n",
      "1512/1512 [==============================] - 132s 87ms/step - loss: 0.5157 - accuracy: 0.7009\n",
      "Epoch 7/10\n",
      "1512/1512 [==============================] - 127s 84ms/step - loss: 0.4909 - accuracy: 0.7166\n",
      "Epoch 8/10\n",
      "1512/1512 [==============================] - 127s 84ms/step - loss: 0.4701 - accuracy: 0.7306\n",
      "Epoch 9/10\n",
      "1512/1512 [==============================] - 130s 86ms/step - loss: 0.4546 - accuracy: 0.7392\n",
      "Epoch 10/10\n",
      "1512/1512 [==============================] - 126s 83ms/step - loss: 0.4397 - accuracy: 0.7484\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train,epochs=100, batch_size=64, verbose=1,callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c2a5056-4392-4dc2-a75e-88954a7006dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 64, 64, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 2) and (None, 64, 64, 1) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mML Data\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mBrainTumorMLtrain\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmodel_name.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;66;03m# Create your model here or load it from somewhere else if it doesn't exist.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Assuming you have a model variable defined.\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m hist \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[callback])\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mML Data\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mBrainTumorMLtrain\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmodel_name100.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(path):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filef_ezv1aw.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\gunta\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 2) and (None, 64, 64, 1) are incompatible\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "path = \"C:\\\\ML Data\\\\BrainTumorMLtrain\\\\model_name.keras\"\n",
    "\n",
    "\n",
    "  # Create your model here or load it from somewhere else if it doesn't exist.\n",
    "    # Assuming you have a model variable defined.\n",
    "hist = model.fit(X_train, Y_train, epochs=100, batch_size=64, verbose=1, callbacks=[callback])\n",
    "model.save(\"C:\\\\ML Data\\\\BrainTumorMLtrain\\\\model_name100.keras\")\n",
    "    \n",
    "\n",
    "if os.path.isfile(path):\n",
    "    model = load_model(path)\n",
    "    \n",
    "    if model is not None:\n",
    "        if hasattr(model, 'history'):\n",
    "            #get_ac = model.history['accuracy']\n",
    "            #get_los = model.history['loss']\n",
    "            loss, accuracy = model.evaluate(X_train, Y_train)\n",
    "        else:\n",
    "            print(\"Model does not have a history attribute.\")\n",
    "    else:\n",
    "        print(\"Failed to load the model from the specified path.\")\n",
    "\n",
    "print(\"Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9627bc8-6211-4abb-add1-2de10fdfd151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
