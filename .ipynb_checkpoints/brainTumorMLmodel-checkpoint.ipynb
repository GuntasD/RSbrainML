{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6264a779-aff5-43c9-856e-d8294103b03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\gunta\\anaconda3\\envs\\my_env\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Downloading scipy-1.10.1-cp38-cp38-win_amd64.whl (42.2 MB)\n",
      "     ---------------------------------------- 0.0/42.2 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/42.2 MB 660.6 kB/s eta 0:01:04\n",
      "     ---------------------------------------- 0.1/42.2 MB 1.1 MB/s eta 0:00:41\n",
      "     ---------------------------------------- 0.3/42.2 MB 2.7 MB/s eta 0:00:16\n",
      "      --------------------------------------- 0.9/42.2 MB 5.0 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 1.5/42.2 MB 7.2 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 3.0/42.2 MB 11.1 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 3.5/42.2 MB 12.5 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 4.5/42.2 MB 12.4 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 5.8/42.2 MB 14.8 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 7.9/42.2 MB 17.3 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 8.1/42.2 MB 16.2 MB/s eta 0:00:03\n",
      "     --------- ----------------------------- 10.0/42.2 MB 18.3 MB/s eta 0:00:02\n",
      "     ---------- ---------------------------- 11.2/42.2 MB 25.2 MB/s eta 0:00:02\n",
      "     ----------- --------------------------- 12.2/42.2 MB 26.2 MB/s eta 0:00:02\n",
      "     ------------ -------------------------- 14.0/42.2 MB 27.3 MB/s eta 0:00:02\n",
      "     ------------- ------------------------- 14.9/42.2 MB 28.4 MB/s eta 0:00:01\n",
      "     --------------- ----------------------- 16.8/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------- ----------------------- 17.1/42.2 MB 26.2 MB/s eta 0:00:01\n",
      "     ---------------- ---------------------- 17.3/42.2 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------ -------------------- 19.5/42.2 MB 27.3 MB/s eta 0:00:01\n",
      "     ------------------ -------------------- 19.5/42.2 MB 27.3 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 21.6/42.2 MB 25.2 MB/s eta 0:00:01\n",
      "     -------------------- ------------------ 22.6/42.2 MB 25.2 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 23.7/42.2 MB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------- ---------------- 24.5/42.2 MB 22.6 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 25.8/42.2 MB 23.4 MB/s eta 0:00:01\n",
      "     ------------------------ -------------- 26.4/42.2 MB 21.1 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 27.5/42.2 MB 21.8 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 28.2/42.2 MB 22.6 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 29.5/42.2 MB 21.1 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 30.7/42.2 MB 22.6 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 31.8/42.2 MB 21.1 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 32.8/42.2 MB 22.6 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 33.9/42.2 MB 22.6 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 35.1/42.2 MB 21.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 36.2/42.2 MB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 37.1/42.2 MB 21.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 38.0/42.2 MB 22.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 39.2/42.2 MB 22.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 39.8/42.2 MB 21.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  41.2/42.2 MB 21.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  41.7/42.2 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 21.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 42.2/42.2 MB 20.5 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.9/9.3 MB 29.4 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.9/9.3 MB 24.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.7/9.3 MB 21.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.4/9.3 MB 20.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.8/9.3 MB 20.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.5/9.3 MB 19.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.8/9.3 MB 19.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.5/9.3 MB 19.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.2/9.3 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.3 MB 18.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 17.4 MB/s eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 302.2/302.2 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.2 scipy-1.10.1 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "# !python --version\n",
    "# !tensorflow --version\n",
    "# !pip install Path\n",
    "# !pip install tensorflow\n",
    "# !pip install opencv-python\n",
    "# !pip install tensorflow-hub\n",
    "# !pip install pydicom\n",
    "#!pip install pandas\n",
    "#!pip install matplotlib\n",
    "#!pip install tqdm\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da819add-471e-4afd-b0cf-3e04426b55d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import cv2 as cv\n",
    "from path import Path\n",
    "import os \n",
    "import glob\n",
    "import tensorflow_hub as hub\n",
    "import os \n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6074f3c-557e-4e3f-999a-f5067586c427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df= pd.read_csv('C:\\\\ML Data\\\\BrainTumorML\\\\train_labels.csv')\n",
    "sample_df = pd.read_csv('C:\\\\ML Data\\\\BrainTumorML\\\\sample_submission.csv')\n",
    "#train_df.head(1)\n",
    "#train_df.tail()\n",
    "#train_df.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51ee1e84-a914-44f4-ae61-604a734dbe7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dicom(path):\n",
    "    dicom=pydicom.read_file(path)\n",
    "    data=dicom.pixel_array\n",
    "    data=data-np.min(data)\n",
    "    if np.max(data) != 0:\n",
    "        data=data/np.max(data)\n",
    "    data=(data*255).astype(np.uint8)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dad50a7-4204-41b5-a97e-fe45023678fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def listdirs(folder):\n",
    "    return [d for d in os.listdir(folder) if os.path.isdir(os.path.join(folder, d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "757d8240-0009-4f0a-8ad1-41ad72967e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = train_df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "595f54d4-3d60-4c9f-8f7a-b5f87f450c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 585/585 [14:38<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "train_dir='C:\\\\ML Data\\\\BrainTumorML\\\\train'\n",
    "trainset=[]\n",
    "trainlabel=[]\n",
    "trainidt=[]\n",
    "for i in tqdm(range(len(train_df))):\n",
    "    idt=train_df.loc[i,'BraTS21ID']\n",
    "    idt2=('00000'+str(idt))[-5:]\n",
    "    path=os.path.join(train_dir,idt2,'T1wCE')              \n",
    "    for im in os.listdir(path):\n",
    "        img=load_dicom(os.path.join(path,im)) \n",
    "        img=cv.resize(img,(64,64)) \n",
    "        image=img_to_array(img)\n",
    "        image=image/255.0\n",
    "        trainset+=[image]\n",
    "        trainlabel+=[train_df.loc[i,'MGMT_value']]\n",
    "        trainidt+=[idt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09a1cdff-c12e-4870-abdd-270251543cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [02:07<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "This is the first testidt 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dir='C:\\\\ML Data\\\\BrainTumorML\\\\test'\n",
    "testset=[]\n",
    "testidt=[]\n",
    "for i in tqdm(range(len(sample_df))):\n",
    "    idt=sample_df.loc[i,'BraTS21ID']\n",
    "    idt2=('00000'+str(idt))[-5:]\n",
    "    path=os.path.join(test_dir,idt2,'T1wCE')               \n",
    "    for im in os.listdir(path):   \n",
    "        img=load_dicom(os.path.join(path,im))\n",
    "        img=cv.resize(img,(64,64)) \n",
    "        image=img_to_array(img)\n",
    "        image=image/255.0\n",
    "        testset+=[image]\n",
    "        testidt+=[idt]\n",
    "print(testidt[0])\n",
    "#print(\"This is the first testIdt\" , testidt[0])\n",
    "firstEX = testidt[0]\n",
    "print(f\"This is the first testidt {firstEX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "399782d4-d3bf-48fb-8b65-ebf2dd99216b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(testidt, open('C:\\\\ML Data\\\\BrainTumorML\\\\test\\\\testIdt.label', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "382832e1-cd88-4082-801e-c334fda2e5b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y=np.array(trainlabel)\n",
    "Y_train=to_categorical(y)\n",
    "X_train=np.array(trainset)\n",
    "X_test=np.array(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36840d38-669b-4b64-a080-047dd90b44f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3024/3024 [==============================] - 45s 15ms/step - loss: 0.2139 - accuracy: 0.8696\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 61, 61, 64)        1088      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 30, 30, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 30, 30, 64)        256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 27, 64)        65600     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 13, 13, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 13, 13, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 13, 13, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 10, 10, 64)        65600     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 5, 5, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 5, 5, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               160100    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 293358 (1.12 MB)\n",
      "Trainable params: 292974 (1.12 MB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n",
      "Successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "path = \"model_100.keras\"\n",
    "\n",
    "\n",
    "  # Create your model here or load it from somewhere else if it doesn't exist.\n",
    "    # Assuming you have a model variable defined.\n",
    "#hist = model.fit(X_train, Y_train, epochs=100, batch_size=64, verbose=1, callbacks=[callback])\n",
    "#model.save(\"C:\\\\ML Data\\\\BrainTumorMLtrain\\\\model_100.keras\")\n",
    "    \n",
    "\n",
    "if os.path.isfile(path):\n",
    "    model = load_model(path)\n",
    "\n",
    "if model is not None:\n",
    "        if hasattr(model, 'history'):\n",
    "            #get_ac = model.history['accuracy']\n",
    "            #get_los = model.history['loss']\n",
    "            loss, accuracy = model.evaluate(X_train, Y_train)\n",
    "    \n",
    "        else:\n",
    "            print(\"Model does not have a history attribute.\")\n",
    "            model = keras.models.Sequential()\n",
    "            model.add(keras.layers.Conv2D(filters=64,kernel_size=(4,4),input_shape=(64,64,1),activation='relu',kernel_initializer=\"he_normal\"))\n",
    "            model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "            model.add(keras.layers.BatchNormalization())\n",
    "            model.add(keras.layers.Conv2D(filters=64,kernel_size=(4,4),activation='relu',kernel_initializer=\"he_normal\"))\n",
    "            model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "            model.add(keras.layers.Dropout(0.20))\n",
    "            model.add(keras.layers.BatchNormalization())\n",
    "            model.add(keras.layers.Conv2D(filters=64,kernel_size=(4,4),activation='relu',kernel_initializer=\"he_normal\"))\n",
    "            model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "            model.add(keras.layers.Dropout(0.25))\n",
    "            model.add(keras.layers.BatchNormalization())\n",
    "            model.add(keras.layers.Flatten())\n",
    "            model.add(keras.layers.Dense(100,activation=\"relu\",kernel_initializer=\"he_normal\"))\n",
    "            model.add(keras.layers.Dense(2,\"softmax\"))\n",
    "            model.compile(loss=\"categorical_crossentropy\", optimizer = \"RMSprop\",metrics=[\"accuracy\"])\n",
    "            callback = keras.callbacks.EarlyStopping(monitor='loss', patience=8)\n",
    "            hist = model.fit(X_train, Y_train,epochs=100, batch_size=64, verbose=1,callbacks=[callback])\n",
    "            model.save(path)\n",
    "else: \n",
    "    print(\"Failed to load the model from the specified path.\")\n",
    "        \n",
    "model.summary()\n",
    "print(\"Successful\")\n",
    "\n",
    "\n",
    "# model = keras.models.Sequential()\n",
    "# model.add(keras.layers.Conv2D(filters=64, kernel_size=(4,4), input_shape=(64,64,1), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "# model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(keras.layers.BatchNormalization())\n",
    "# model.add(keras.layers.Conv2D(filters=64, kernel_size=(4,4), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "# model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(keras.layers.BatchNormalization())\n",
    "# model.add(keras.layers.Dropout(0.20))\n",
    "# model.add(keras.layers.Conv2D(filters=64, kernel_size=(4,4), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "# model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(keras.layers.Dropout(0.25))\n",
    "# model.add(keras.layers.BatchNormalization())\n",
    "# model.add(keras.layers.Flatten())\n",
    "# model.add(keras.layers.Dropout(0.20))\n",
    "# model.add(keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "# model.add(keras.layers.Dropout(0.25))\n",
    "# model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89044cf-d44e-4c50-8e1a-949c7f0e2268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
